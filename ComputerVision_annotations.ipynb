{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUy6_TepHm8r"
      },
      "outputs": [],
      "source": [
        "#we can use roboflox for annotation creation\n",
        "\n",
        "#we can save data in coco json format and convert this into yolo style format using roboflox\n",
        "#but if have bit map annotation then that is difficult to do with these tools\n",
        "#we can convert bitmaps to coco style then to yolo style\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Part 1: Convert binary annotations to COCO JSON\n",
        "\n",
        "For each binary mask, the code extracts contours using OpenCV.\n",
        "These contours represent the boundaries of objects within the images. This is a key step in converting binary masks to\n",
        "polygon-like annotations.\n",
        "\n",
        "Then, convert the contours into annotations, including bounding boxes, area, and segmentation information.\n",
        "Each annotation is associated with an image ID, category ID, and other properties required by the COCO format.\n",
        "\n",
        "The code also creates an images section containing metadata about the images, such as their filenames, widths,\n",
        "and heights. In my example, I have used exactly the same file names for all images and masks so that a given mask\n",
        "can be easily mapped to the image.\n",
        "\n",
        "All the annotations, images, and categories are assembled into a dictionary that follows the COCO JSON format.\n",
        "This includes sections for \"info,\" \"licenses,\" \"images,\" \"categories,\" and \"annotations.\"\n",
        "\n",
        "Finally, the assembled COCO JSON data is saved to a file, making it ready to be used with tools and frameworks\n",
        "that support the COCO data format.\n",
        "\n",
        "Part 2: Converting COCO JSON annotation to YOLO v8\n",
        "\n",
        "It reads coco style json annotations supplied as a single json file and also\n",
        "images as input.\n",
        "\n",
        "Here are the key steps in the code:\n",
        "\n",
        "1. Convert Images to YOLO Format: The convert_to_yolo function takes paths for input images and annotations\n",
        "(in JSON format), and directories to store the output images and labels. It then performs the following operations:\n",
        "- Reads the input JSON file containing annotations.\n",
        "- Copies all PNG images from the input directory to the output directory.\n",
        "- Normalizes the polygon segmentation data related to each image and writes them to text files, mapping them to the\n",
        " appropriate category (e.g., Alpha, Cells, Mito, Vessels).\n",
        "- The resulting text files contain information about the object category and the normalized coordinates of the\n",
        "polygons that describe the objects.\n",
        "\n",
        "2. Create YAML Configuration File: The create_yaml function takes paths to the input JSON file containing categories,\n",
        " training, validation, and optional test paths. It then:\n",
        "- Extracts the category names and the number of classes.\n",
        "- Constructs a dictionary containing information about class names, the number of classes, and paths to the training,\n",
        "validation, and test datasets.\n",
        "- Writes this dictionary to a YAML file, which can be used as a configuration file for\n",
        "training a model (e.g., a YOLO model).\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "nTxQtgE5I728",
        "outputId": "df05e559-caab-40c2-cfe1-77e32c855999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Part 1: Convert binary annotations to COCO JSON\\n\\nFor each binary mask, the code extracts contours using OpenCV. \\nThese contours represent the boundaries of objects within the images. This is a key step in converting binary masks to \\npolygon-like annotations. \\n\\nThen, convert the contours into annotations, including bounding boxes, area, and segmentation information. \\nEach annotation is associated with an image ID, category ID, and other properties required by the COCO format.\\n\\nThe code also creates an images section containing metadata about the images, such as their filenames, widths, \\nand heights. In my example, I have used exactly the same file names for all images and masks so that a given mask \\ncan be easily mapped to the image. \\n\\nAll the annotations, images, and categories are assembled into a dictionary that follows the COCO JSON format. \\nThis includes sections for \"info,\" \"licenses,\" \"images,\" \"categories,\" and \"annotations.\"\\n\\nFinally, the assembled COCO JSON data is saved to a file, making it ready to be used with tools and frameworks \\nthat support the COCO data format.\\n\\nPart 2: Converting COCO JSON annotation to YOLO v8 \\n\\nIt reads coco style json annotations supplied as a single json file and also \\nimages as input. \\n\\nHere are the key steps in the code:\\n\\n1. Convert Images to YOLO Format: The convert_to_yolo function takes paths for input images and annotations \\n(in JSON format), and directories to store the output images and labels. It then performs the following operations:\\n- Reads the input JSON file containing annotations.\\n- Copies all PNG images from the input directory to the output directory.\\n- Normalizes the polygon segmentation data related to each image and writes them to text files, mapping them to the\\n appropriate category (e.g., Alpha, Cells, Mito, Vessels).\\n- The resulting text files contain information about the object category and the normalized coordinates of the \\npolygons that describe the objects.\\n\\n2. Create YAML Configuration File: The create_yaml function takes paths to the input JSON file containing categories,\\n training, validation, and optional test paths. It then:\\n- Extracts the category names and the number of classes.\\n- Constructs a dictionary containing information about class names, the number of classes, and paths to the training, \\nvalidation, and test datasets.\\n- Writes this dictionary to a YAML file, which can be used as a configuration file for \\ntraining a model (e.g., a YOLO model).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### for converting bitmap to coco style"
      ],
      "metadata": {
        "id": "3aB0kUu4LfBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import glob\n",
        "# import json\n",
        "# import os\n",
        "# import cv2\n",
        "\n",
        "# # Label IDs of the dataset representing different categories\n",
        "# category_ids = {\n",
        "#     \"Alpha\": 1,\n",
        "#     \"Cells\": 2,\n",
        "#     \"Mito\": 3,\n",
        "#     \"Vessels\": 4,\n",
        "# }\n",
        "\n",
        "# MASK_EXT = 'png'\n",
        "# ORIGINAL_EXT = 'png'\n",
        "# image_id = 0\n",
        "# annotation_id = 0\n",
        "\n",
        "# def images_annotations_info(maskpath):\n",
        "#     \"\"\"\n",
        "#     Process the binary masks and generate images and annotations information.\n",
        "\n",
        "#     :param maskpath: Path to the directory containing binary masks\n",
        "#     :return: Tuple containing images info, annotations info, and annotation count\n",
        "#     \"\"\"\n",
        "#     global image_id, annotation_id\n",
        "#     annotations = []\n",
        "#     images = []\n",
        "\n",
        "#     # Iterate through categories and corresponding masks\n",
        "#     for category in category_ids.keys():\n",
        "#         for mask_image in glob.glob(os.path.join(maskpath, category, f'*.{MASK_EXT}')):\n",
        "#             original_file_name = f'{os.path.basename(mask_image).split(\".\")[0]}.{ORIGINAL_EXT}'\n",
        "#             mask_image_open = cv2.imread(mask_image)\n",
        "\n",
        "#             # Get image dimensions\n",
        "#             height, width, _ = mask_image_open.shape\n",
        "\n",
        "#             # Create or find existing image annotation\n",
        "#             if original_file_name not in map(lambda img: img['file_name'], images):\n",
        "#                 image = {\n",
        "#                     \"id\": image_id + 1,\n",
        "#                     \"width\": width,\n",
        "#                     \"height\": height,\n",
        "#                     \"file_name\": original_file_name,\n",
        "#                 }\n",
        "#                 images.append(image)\n",
        "#                 image_id += 1\n",
        "#             else:\n",
        "#                 image = [element for element in images if element['file_name'] == original_file_name][0]\n",
        "\n",
        "#             # Find contours in the mask image\n",
        "#             gray = cv2.cvtColor(mask_image_open, cv2.COLOR_BGR2GRAY)\n",
        "#             _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "#             contours = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
        "\n",
        "#             # Create annotation for each contour\n",
        "#             for contour in contours:\n",
        "#                 bbox = cv2.boundingRect(contour)\n",
        "#                 area = cv2.contourArea(contour)\n",
        "#                 segmentation = contour.flatten().tolist()\n",
        "\n",
        "#                 annotation = {\n",
        "#                     \"iscrowd\": 0,\n",
        "#                     \"id\": annotation_id,\n",
        "#                     \"image_id\": image['id'],\n",
        "#                     \"category_id\": category_ids[category],\n",
        "#                     \"bbox\": bbox,\n",
        "#                     \"area\": area,\n",
        "#                     \"segmentation\": [segmentation],\n",
        "#                 }\n",
        "\n",
        "#                 # Add annotation if area is greater than zero\n",
        "#                 if area > 0:\n",
        "#                     annotations.append(annotation)\n",
        "#                     annotation_id += 1\n",
        "\n",
        "#     return images, annotations, annotation_id\n",
        "\n",
        "\n",
        "# def process_masks(mask_path, dest_json):\n",
        "#     global image_id, annotation_id\n",
        "#     image_id = 0\n",
        "#     annotation_id = 0\n",
        "\n",
        "#     # Initialize the COCO JSON format with categories\n",
        "#     coco_format = {\n",
        "#         \"info\": {},\n",
        "#         \"licenses\": [],\n",
        "#         \"images\": [],\n",
        "#         \"categories\": [{\"id\": value, \"name\": key, \"supercategory\": key} for key, value in category_ids.items()],\n",
        "#         \"annotations\": [],\n",
        "#     }\n",
        "\n",
        "#     # Create images and annotations sections\n",
        "#     coco_format[\"images\"], coco_format[\"annotations\"], annotation_cnt = images_annotations_info(mask_path)\n",
        "\n",
        "#     # Save the COCO JSON to a file\n",
        "#     with open(dest_json, \"w\") as outfile:\n",
        "#         json.dump(coco_format, outfile, sort_keys=True, indent=4)\n",
        "\n",
        "#     print(\"Created %d annotations for images in folder: %s\" % (annotation_cnt, mask_path))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     train_mask_path = \"EM-platelet-multi/input/train_masks/\"\n",
        "#     train_json_path = \"EM-platelet-multi/input/train_images/train.json\"\n",
        "#     process_masks(train_mask_path, train_json_path)\n",
        "\n",
        "#     val_mask_path = \"EM-platelet-multi/input/val_masks/\"\n",
        "#     val_json_path = \"EM-platelet-multi/input/val_images/val.json\"\n",
        "#     process_masks(val_mask_path, val_json_path)"
      ],
      "metadata": {
        "id": "YZUztIVCLSKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### for converting coco style to yolo style"
      ],
      "metadata": {
        "id": "dXspbO0RLlKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import os\n",
        "# import shutil\n",
        "# import yaml\n",
        "\n",
        "# # Function to convert images to YOLO format\n",
        "# def convert_to_yolo(input_images_path, input_json_path, output_images_path, output_labels_path):\n",
        "#     # Open JSON file containing image annotations\n",
        "#     f = open(input_json_path)\n",
        "#     data = json.load(f)\n",
        "#     f.close()\n",
        "\n",
        "#     # Create directories for output images and labels\n",
        "#     os.makedirs(output_images_path, exist_ok=True)\n",
        "#     os.makedirs(output_labels_path, exist_ok=True)\n",
        "\n",
        "#     # List to store filenames\n",
        "#     file_names = []\n",
        "#     for filename in os.listdir(input_images_path):\n",
        "#         if filename.endswith(\".png\"):\n",
        "#             source = os.path.join(input_images_path, filename)\n",
        "#             destination = os.path.join(output_images_path, filename)\n",
        "#             shutil.copy(source, destination)\n",
        "#             file_names.append(filename)\n",
        "\n",
        "#     # Function to get image annotations\n",
        "#     def get_img_ann(image_id):\n",
        "#         return [ann for ann in data['annotations'] if ann['image_id'] == image_id]\n",
        "\n",
        "#     # Function to get image data\n",
        "#     def get_img(filename):\n",
        "#         return next((img for img in data['images'] if img['file_name'] == filename), None)\n",
        "\n",
        "#     # Iterate through filenames and process each image\n",
        "#     for filename in file_names:\n",
        "#         img = get_img(filename)\n",
        "#         img_id = img['id']\n",
        "#         img_w = img['width']\n",
        "#         img_h = img['height']\n",
        "#         img_ann = get_img_ann(img_id)\n",
        "\n",
        "#         # Write normalized polygon data to a text file\n",
        "#         if img_ann:\n",
        "#             with open(os.path.join(output_labels_path, f\"{os.path.splitext(filename)[0]}.txt\"), \"a\") as file_object:\n",
        "#                 for ann in img_ann:\n",
        "#                     current_category = ann['category_id'] - 1\n",
        "#                     polygon = ann['segmentation'][0]\n",
        "#                     normalized_polygon = [format(coord / img_w if i % 2 == 0 else coord / img_h, '.6f') for i, coord in enumerate(polygon)]\n",
        "#                     file_object.write(f\"{current_category} \" + \" \".join(normalized_polygon) + \"\\n\")\n",
        "\n",
        "# # Function to create a YAML file for the dataset\n",
        "# def create_yaml(input_json_path, output_yaml_path, train_path, val_path, test_path=None):\n",
        "#     with open(input_json_path) as f:\n",
        "#         data = json.load(f)\n",
        "\n",
        "#     # Extract the category names\n",
        "#     names = [category['name'] for category in data['categories']]\n",
        "\n",
        "#     # Number of classes\n",
        "#     nc = len(names)\n",
        "\n",
        "#     # Create a dictionary with the required content\n",
        "#     yaml_data = {\n",
        "#         'names': names,\n",
        "#         'nc': nc,\n",
        "#         'test': test_path if test_path else '',\n",
        "#         'train': train_path,\n",
        "#         'val': val_path\n",
        "#     }\n",
        "\n",
        "#     # Write the dictionary to a YAML file\n",
        "#     with open(output_yaml_path, 'w') as file:\n",
        "#         yaml.dump(yaml_data, file, default_flow_style=False)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     base_input_path = \"EM-platelet-multi/input/\"\n",
        "#     base_output_path = \"EM-platelet-multi/yolo_dataset/\"\n",
        "\n",
        "#     # Processing validation dataset (if needed)\n",
        "#     convert_to_yolo(\n",
        "#         input_images_path=os.path.join(base_input_path, \"val_images\"),\n",
        "#         input_json_path=os.path.join(base_input_path, \"val_images/val.json\"),\n",
        "#         output_images_path=os.path.join(base_output_path, \"valid/images\"),\n",
        "#         output_labels_path=os.path.join(base_output_path, \"valid/labels\")\n",
        "#     )\n",
        "\n",
        "#     # Processing training dataset\n",
        "#     convert_to_yolo(\n",
        "#         input_images_path=os.path.join(base_input_path, \"train_images\"),\n",
        "#         input_json_path=os.path.join(base_input_path, \"train_images/train.json\"),\n",
        "#         output_images_path=os.path.join(base_output_path, \"train/images\"),\n",
        "#         output_labels_path=os.path.join(base_output_path, \"train/labels\")\n",
        "#     )\n",
        "\n",
        "#     # Creating the YAML configuration file\n",
        "#     create_yaml(\n",
        "#         input_json_path=os.path.join(base_input_path, \"train_images/train.json\"),\n",
        "#         output_yaml_path=os.path.join(base_output_path, \"data.yaml\"),\n",
        "#         train_path=\"EM-Platelet/train/images\",\n",
        "#         val_path=\"EM-Platelet/valid/images\",\n",
        "#         test_path='../test/images'  # or None if not applicable\n",
        "#     )"
      ],
      "metadata": {
        "id": "G0kgrpyALbm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### display yolo annotations"
      ],
      "metadata": {
        "id": "wvR4Zi-ONsRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.patches as patches\n",
        "# import cv2\n",
        "\n",
        "# def display_image_with_annotations(image_path, annotation_path, colors=None):\n",
        "#     # Load image using OpenCV and convert it from BGR to RGB color space\n",
        "#     image = cv2.imread(image_path)\n",
        "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#     img_h, img_w, _ = image.shape\n",
        "\n",
        "#     # Create a figure and axis to display the image\n",
        "#     fig, ax = plt.subplots(1)\n",
        "#     ax.imshow(image)\n",
        "#     ax.axis('off')  # Turn off the axes\n",
        "\n",
        "#     # Define a default color map if none is provided\n",
        "#     if colors is None:\n",
        "#         colors = plt.cm.get_cmap('tab10')\n",
        "\n",
        "#     # Open the annotation file and process each line\n",
        "#     with open(annotation_path, 'r') as file:\n",
        "#         for line in file:\n",
        "#             parts = line.strip().split()\n",
        "#             category_id = int(parts[0])\n",
        "#             # Choose color based on category ID, looping through color map if more than 10 categories\n",
        "#             color = colors(category_id % 10)\n",
        "#             # Extract normalized polygon coordinates and denormalize them\n",
        "#             polygon = [float(coord) for coord in parts[1:]]\n",
        "#             polygon = [coord * img_w if i % 2 == 0 else coord * img_h for i, coord in enumerate(polygon)]\n",
        "#             # Reshape into (num_points, 2) array\n",
        "#             polygon = [(polygon[i], polygon[i+1]) for i in range(0, len(polygon), 2)]\n",
        "#             # Create a Polygon patch using the denormalized coordinates\n",
        "#             patch = patches.Polygon(polygon, closed=True, edgecolor=color, fill=False)\n",
        "#             # Add the patch to the plot to display the annotated region\n",
        "#             ax.add_patch(patch)\n",
        "\n",
        "#     plt.show()  # Display the image with annotations\n",
        "\n",
        "# # Example usage with specified image and annotation paths\n",
        "# image_path = \"EM-platelet-multi/yolo_dataset/train/images/3D-EM-platelet-train04.png\"\n",
        "# annotation_path = \"EM-platelet-multi/yolo_dataset/train/labels/3D-EM-platelet-train04.txt\"\n",
        "# display_image_with_annotations(image_path, annotation_path)"
      ],
      "metadata": {
        "id": "vANCvMcvLyp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we need to have yaml file to work with yolo"
      ],
      "metadata": {
        "id": "bWOLfHieNvmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4g1WRhrEPC77"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}